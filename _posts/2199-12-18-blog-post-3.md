---
title: "Learning Note on \"Information Lattice Learning\""
date: 2199-12-18
permalink: /posts/2199/12/ILL
excerpt_separator: <!--more-->
toc: true
tags:
  - learning note
---
[Information Lattice Learning](https://www.jair.org/index.php/jair/article/view/14277) (ILL) is a journal paper published in 2023 trying to address the explainability challenge of AI. Instead of predicting data X and target Y, it focuses on explaining so called "what makes X an X" (or "what makes X differ from Y"). <!--more--> Interestingly, the "rules" of the signal of this work seems to be only interpretable to domain experts, but not for everyone. This also reflects one of the notion of interpretability considered in the paper - **interpretable to whom**. This learning note is another layer of interpretation yet hopefully "easier" to understand.

<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [Background](#background)
- [Terminology](#terminology)
    - [**Signal**](#signal)
    - [**Partition**](#partition)
    - [**Rules**](#rules)
    - [**Partition lattice**](#partition-lattice)
    - [**Information lattice**](#information-lattice)
    - [**Projection**](#projection)
    - [**Lifting**](#lifting)
- [What does Information Lattice Learning (ILL) mean?](#what-does-information-lattice-learning-ill-mean)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

# Background
Often traditional unsupervised learning techniques such as clustering and dimensionality reduction don't give good human-level interpretation. For a given dataset samples, clustering has no knowledge of absent data outside of the seen dataset, and forming present data-specific clusters that has no special human-level reasons to explain. Dimensionality reductions similarly provide no explanation of what do the reduced dimensions mean, and how to interpret and evaluate on new data is not clear. ILL states that knowledge discovery is not just compression and clustering.

# Terminology
It's not hard to find out that one of the difficulties of digesting this paper is understanding and memorizing the terminology and symbols. You may need to double-check alien terms back and forth while reading. Just like the case where at some point you need to go back and check what does ILL mean throughout this note. Annoying, isn't it? Somehow this is the way how human convey ideas. And ILL provides food for thought.

Let's jump to terminology part and summarize them in the following.

### **Signal**
A function <span style="color:red">**ξ** : **X** &rarr; **ℝ**</span>, and also represent any probability distribution. **ξ** (xi) is signal, **X** (Chi) is domain. One example is **X** ⊆ **ℝ**<sup>**n**</sup>.

### **Partition**

A partition **P** of a set **X** to denote an **abstraction** of **X**. For example, the following abstractions of the six outcomes of a die roll
\{\{1,2\},\{3,4\},\{5,6\}\}, \{\{1,2,3,4\},\{5,6\}\}, \{\{1,3,5\},\{2,4,6\}\} are partitions of the set **X**= \{1,2,3,4,5,6\}.

### **Rules**
A **rule** of a signal is a "coarsened" (or merged, collectively) signal <span style="color:red">**r**<sub>**ξ**</sub> : **P** &rarr; **ℝ**</span> on a partition **P** of **X**.

### **Partition lattice**
The **partially ordered set**(or called **poset**) containing all partitions of **X**, denoted as <span style="color:red">(**β**<sub>**X**</sub>,≼)</span>.

### **Information lattice**

The **information lattice** of a **signal** is the **poset** of all **rules** of that **signal**. Also, an **information lattice** of a **signal** is **isomorphic** to the underlying **partition lattice** via **projection**.

> A sentence like this contains too much alien terms to understand, and effectively explains nothing. One learning goal is to make this sentence sensible after explanation.

### **Projection**
An operator that projects a **signal** down to a **rule**.

### **Lifting**
An operator that lifts **rule(s)** up to a **signal**.

# What does Information Lattice Learning (ILL) mean?
ILL is a single optimization problem. Given a signal we want to explain, explaining means to search for a rule set such that it recovers (by projection) the signal well, and the rule set is simpler (i.e. it contains fewer and simpler rules). The search space involves the full information lattice, or isomorphically, the full partition lattice. Formal definition is skipped here. Note that a recovery may not be exact, a loss function to measure the mismatch of signal and recovered one is also introduced.

Due to not only computational concerns but also different level of interpretabilities of criterions an abstraction can made, computing all partitions of **X** is unrealistic. Practically, ILL can be achieved via 2 phases: (1) lattice construction and (2) lattice learning. Which I think is the key novelty of this work.